---
title: 'Writing an NIH Grant in 9 "easy" steps'
subtitle: 'Step 5: Research Strategy'
author: 'Jonathan Peelle'
institute: 'Northeastern University <br> <a href="http://neuromatch.social/@jpeelle">@jpeelle@neuromatch.social</a>'
title-slide-attributes:
    data-background-image: images/stairs.jpg
    data-background-size: cover
    data-background-opacity: '0.6'
format:
  revealjs: 
    theme: default
    slide-number: true
    chalkboard: 
      buttons: false
    preview-links: auto
    css: styles.css
---



## Homework updates and questions {background-image="images/isec01.jpg" background-repeat="no-repeat" background-opacity=0.5}



## Research Strategy

 - After your aims, the most important part of your grant
 - Improves through iteration, feedback, and experience
 - Important to focus both on *content* (scientific ideas) and *presentation* (how you communicate these ideas, particularly aesthetic and organizational choices)


# Part 1: Content

## Organization

 - Standard sections

 - Everyone allocates space differently and there are many effective ways to structure a grant, but remember your jobs: *make the reviewer's job easy*
    - Tell them what they need to know to understand your grant
    - Highlight what you anticipate are score-driving factors (particularly significance and innovation)

## A suggested organization

- Significance (half page)
- Innovation (half page)
- Background
  - Tell reviewers everything they need to know about your study
  - Highlight your past work as appropriate
- General methodology (if anything common across aims)
  - Subject selection, data acquisition, assessments...
- Details of each aim (preliminary data under each aim)

## Alternate organization 1

- Significance and Background together and long
- Preliminary data for all aims
- General methodology
- Details of each aim

As a reviewer I prefer the details and preliminary data for each aim near each other: I read the method, wonder if it works, and then *BAM*: "Figure 8 shows preliminary data for this aim proving it works"

## Headings

 - You can have whatever subheadings you want
    - Use these to help reviewers, especially the ones who might be skimming
    - Impeccable nested numbering does not prevent reviewers from getting confused



## What methods do you include?

 - You can't include everything, but:
 - Reviewers should be able to tell what you are actually doing
 - Some details are important for assessing feasibility and/or power
   - number of participants
   - number of trials
   - what the stimuli and task are

## Sample size

 - In the real world, small sample sizes and poor reliability are enormous problems
 - These are often (though perhaps not always) overlooked at study section
   - A lot of "rule of thumb" and gut instincts drive reviewer comments

## Sample size

**DO**:

 - Make your best effort
 - Use real numbers (means, variance)
 - Read the statistical literature
 - Consult a statician if necessary (early)
 - Err on the side of too many participants
 - Account for attrition

## Sample size

**DON'T**:

 - Be vague ("We run enough participants to detect an effect of 0.8, p < .05, calculated using GPower")


## Sample size

- One approach I have taken that has worked in grants, and also to some degree in my lab, is Bayes Factor Design Analysis ([SchÃ¶nbrodt and Wagenmakers, 2018](https://link.springer.com/article/10.3758/s13423-017-1230-y))
  - Sequential sampling with optional stopping structured around the minimal sample size required for informative results


## Sample size behavioral example

::: {style="font-size:.8em"}
We will use Bayes Factor Design Analysis (BFDA) to determine sample sizes for behavioral testing (SchÃ¶nbrodt and Wagenmakers, 2018). BFDA is a form of sequential sampling that involves testing until sufficient evidence is collected for, or against, a hypothesis. Note that optional stopping does not bias interpretation of statistics (Rouder, 2014). For every experiment, we will run at least 80 participants per group, and continue until the main outcome reaches a Bayes Factor of â‰¥ 10 or â‰¤ 0.1 (moderate evidence for or against the hypothesis). For planning purposes, we use a traditional multiple regression approach after Maxwell (2000). We assume a correlation of 0.3 between three primary predictor variables (age, hearing level, reward sensitivity), and a correlation of 0.3 between reward sensitivity and speech perception. Entering these into eq. 18 of Maxwell (2000) results in a minimum sample size of 218. We plan for 240 from each age group to account for anticipated 10% attrition. This results in 480 participants for each behavioral experiment (1440 participants). 
:::

## Sample size fMRI example

::: {style="font-size:.8em"}
Regarding power for fMRI studies, one question is how many participants we need for a significant group effect. For this, we used a prior fMRI study of reward (Wimmer et al., 2012) available on NeuroVault, and the power calculation technique by Durnez et al. (2016), which indicates 36 participants for a power of 0.9. We will run 36 young adults for each fMRI study, planning to recruit 40 to allow for data loss. For the older adults, we are interested in individual differences. Yarkoni and Braver (2010) argue that fMRI studies of individual differences are typically underpowered, and to achieve power of 0.8 for even a large correlation effect requires a minimum of 55 subjects. We will run 72 older adults in all fMRI studies, giving us power of > 0.9 to detect effects reported by Peelle et al. (2011) in a correlation study of hearing and fMRI activity based on ROI-calculated effect sizes (Pearson r = -0.65 in auditory cortex). We anticipate 10% data loss (movement etc.), 
:::

## Alternate outcomes

 - Each experiment should have alternate outcomes. What happens if you don't get the result you predict?
 - I call this "Challenges, potential pitfalls, or alternate outcomes" and then I pick which of those I want to address. ðŸ˜¬
 - ("We are unlikely to get an alternate outcome" is not very compelling)


## Feasibility

**Can you do what you say you will do?**

 - Number of participants
   - Recruitment from particular groups (e.g. patient popultions)
 - Analysis methods (do you propose __________ but have never done this before?)
 - Managing the budget/personnel

## Addressing Feasibility

As much as you can, anticipate what feasibility concerns reviewers might have (founded, or not) and be proactive about addressing them:

 - Co-investigators / consultants
 - Letters of support
 - Prior publications
 - Preliminary data


## Preliminary data

Preliminary data can serve different purposes:

1. Establish feasibility
2. Demonstrate expertise (see also #1)
3. Help the reviewers understand something



## Preliminary data

Different kinds of preliminary data might serve different purposes:

 - A published figure from a manuscript in your lab can show your expertise and/or help reviewers understand something
 - A letter of support might be reassuring regarding recruitment or analyses
 - Data from a pilot study shows how the specific thing you are proposing might turn out
   - Reviewers vary in how closely they want preliminary data to match the specifics of what you propose. Sometimes you have to make do with what you have available.
  
## Content: Conclusions

 - Remember your goal (communication) and your audience (tired grant reviewers who aren't as expert in your niche as you think they are)
 - Be prepared for a roller coaster with 12 pages
   - Early on: oh my so many blank pages
   - Midway on: I can do this
   - Near the end: 12 pages!
   - Nearer the end: Oh no 15 pages
   - The very end: Somehow I managed to cut it down to 12 pages


# Concluding remarks

## Summary

 - Start early and iterate
 - Be inspired by other people's grants

## Questions? {background-image="images/jp01.jpg" background-repeat="no-repeat"}



## Homework

 - Get example grant materials from someone (which could be me)
 - Draft at least facilities and equipment sections (or why not just do everything)


## Next time {background-image="images/huskies01.jpg" background-repeat="no-repeat" background-opacity=0.5}

Research strategy!
